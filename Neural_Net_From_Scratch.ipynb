{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Net_From_Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SD0313/StartOnAI/blob/master/Neural_Net_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwYJsb6MjDbW",
        "colab_type": "text"
      },
      "source": [
        "# **Neural Network from Scratch Tutorial in Python**\n",
        "###### Created by **(Karthik Bhargav, Keshav Shah, Sauman Das)** for [StartOnAI](https://startonai.com/)\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbqPIhRQNWWF",
        "colab_type": "text"
      },
      "source": [
        "#Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srB7hSW32oJu",
        "colab_type": "text"
      },
      "source": [
        "We will cover the following topics in this notebook.\n",
        "\n",
        "\n",
        "*   Theory of how Perceptrons work and Learn\n",
        "*   Coded Walkthrough of a Neural Network\n",
        "*   Model the Wisconsin Breast Cancer Dataset\n",
        "*   Review Various Applications of Neural Networks\n",
        "\n",
        "So stay tuned!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOr3gaeovftN",
        "colab_type": "text"
      },
      "source": [
        "# What Are Neural Networks?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESAIluzExhen",
        "colab_type": "text"
      },
      "source": [
        "In simple terms, neural networks are representative of the human brain, and they are specificially made to recognize patterns. They interpret data through various models. The patterns that these models detect are all numerical specifically in the form of vectors. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SamhlETV1ZTd",
        "colab_type": "text"
      },
      "source": [
        "Neural networks are extremely helpful for performing tasks involving clustering and classification. Because of the networks similarity to the human brain, it is able to recognize patterns in unlabeled data.\n",
        "\n",
        "We will start off by investigating the most basic Nueral Network: **The Perceptron**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PElDyoWNlFOv",
        "colab_type": "text"
      },
      "source": [
        "## Perceptrons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3dDubF6LUsh",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://tinyurl.com/ybcfd78e\" alt=\"perceptron\" width=\"400\"/>\n",
        "\n",
        "[1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFvT_Nv-AXCN",
        "colab_type": "text"
      },
      "source": [
        "The Perceptron consists of two main components\n",
        "1.   Neurons ($x_i$)\n",
        "2.   Weights ($w_i$)\n",
        "\n",
        "Perceptrons represent the most basic form of a Neural Network with only two layers, the input and output layer.  As shown in the diagram above, both layers are joined by weights represented by the arrows. Each individual neuron represents a number. For example, if there are three inputs, the input layer will consist of 3 neurons plus an additional bias neuron. The importance of the bias ($b$) will become clear later in this tutorial. The output layer simply consists of one neuron in this scenario which represents the number we are attempting to predict. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyhmPbDzUspC",
        "colab_type": "text"
      },
      "source": [
        "**Forward Propagation**\n",
        "\n",
        "The process of going from the input layer to the output is known as Forward Propagation. To simplify the computations, we will use vector notation to represent the input features and the weights.\n",
        "\n",
        "  $\\vec{x}=\\begin{bmatrix}  x_1 & x_2 & ... & x_n\\end{bmatrix}$\n",
        "\n",
        "\n",
        "  $\\vec{w}=\\begin{bmatrix}  w_1 & w_2 & ... & w_n \\end{bmatrix}$\n",
        "\n",
        "  Finally, to get the value of the output neuron, we simply take the dot product of these two vectors and add the bias. \n",
        "\n",
        "  $z=\\vec{x}\\cdot\\vec{w}+b=x_1\\times w_1+x_2\\times w_2+...+x_n\\times w_n+b$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibfm7azdc5-2",
        "colab_type": "text"
      },
      "source": [
        "**The Bias Term**\n",
        "\n",
        "To get a better understanding of this output, lets analyze it with just one input neuron. In other words, our output neuron will store the following.\n",
        "\n",
        "$z=x_1\\times w_1+b$\n",
        "\n",
        "If we visualize this in two dimensional space, we know that this will represent a line with slope $w_1$ and intercept $b$. We can now easily see the role of the bias. Without it, our model would always go through the origin. Now, we can shift our model along the axes giving us more flexibility while training. However, we are still only able to represent linear models. To add non-linearities to our model we use an activation function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJdJxEI9hf9T",
        "colab_type": "text"
      },
      "source": [
        "**Activation Functions**\n",
        "\n",
        "Lets imagine that we are solving a binary classification problem. This means the range of our output $\\hat{y}$ (predicted value) must be $(0, 1)$ since we are predicting a probablity that the input belongs to a certain class. However, the range of a linear equation is $(-\\infty, \\infty)$. Therefore, we must apply some other function to satisfy this constraint. In binary classification problems, the most common activation function is called the sigmoid function. \n",
        "\n",
        "$\\sigma(x)=\\frac{1}{1+e^{-x}}$\n",
        "\n",
        "\n",
        "<img src=\"https://tinyurl.com/ycggxehs\" alt=\"sigmoid_graph\" width=\"400\"/>\n",
        "\n",
        "As you can see in this graph, $\\sigma(x)\\in(0, 1)$. This activation function makes it possible to predict a probablity for a binary output. As you go further into machine learning, you will see several other activation functions. The most common ones other than sigmoid are ReLU, tanh, and softmax.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB2F37Lp-jHU",
        "colab_type": "text"
      },
      "source": [
        "**The Output**\n",
        "\n",
        "Now that we know all the parts of the perceptron, lets see how to get the final output. After forward propogation, we saw the output was\n",
        "\n",
        "  $z=\\vec{x}\\cdot\\vec{w}+b=x_1\\times w_1+x_2\\times w_2+...+x_n\\times w_n+b$\n",
        "\n",
        "Finally, we must apply the activation function to get our final output.\n",
        "\n",
        "$\\hat{y}=\\sigma(z)$\n",
        "\n",
        "That is all there is to getting the output from a perceptron! To sum it up in three simple steps:\n",
        "\n",
        "\n",
        "\n",
        "1.   Get the dot product of the weights and the input features $(\\vec{x}\\cdot\\vec{w})$.\n",
        "2.   Add the bias $(\\vec{x}\\cdot\\vec{w}+b)$.\n",
        "3.   Apply the activation function and that is the predicted value $(\\hat{y}=\\sigma(\\vec{x}\\cdot\\vec{w}+b))$!\n",
        "\n",
        "So far we know how to take the input values, and return the corresponding output. However, we must adjust the weights to make the network fit the training data. The process of making these adjustments is known as **back propagation**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g-R2gHbEw5R",
        "colab_type": "text"
      },
      "source": [
        "**Loss Function**\n",
        "\n",
        "In order to adjust our weights, first we must figure out a way to numerically signify the accuracy of our prediction. In other words, we need to figure out how close our predicted value to the actual value. Several functions exist for accomplishing this task, however, the most common loss function for binary problems is called **Binary Cross-Entropy**.\n",
        "\n",
        "$\\mathcal{L}(y, \\hat{y})=-(y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y}))$\n",
        "\n",
        "Where $y$ is the actual value (0 or 1) and $\\hat{y}$ is the predicted probablity. Looking closer at this equation, we can see that the first term will cancel out if $y=0$ and similarly the second term will cancel out if $y=1$. Therefore, we can write the same equation as a piecewise function.\n",
        "\n",
        "$\\mathcal{L}(y, \\hat{y})=\\begin{cases}-\\log(1-\\hat{y}) & \\text{if $y=0$} \\\\-\\log(\\hat{y}) & \\text{if $y=1$}\\end{cases}$\n",
        "\n",
        "Keep in mind that $\\hat{y}$ is a decimal value in the range $(0, 1)$. The $\\log$ function returns a negative number for such values. As a result, we must take the negative of the log to return a positive value. \n",
        "\n",
        "To see why this function works as the error, try experimenting in the next code cell with different values of $y$ and $\\hat{y}$ then analyze the corresponding loss function value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YdpLQSFPUdk",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "545f153b-282b-4bc0-f4d1-34b41dcd468c"
      },
      "source": [
        "import numpy as np\n",
        "def binary_crossentropy(y, yhat):\n",
        "  #code is derived from the piecewise function\n",
        "  if y == 0:\n",
        "    return -np.log(1.0-yhat)\n",
        "\n",
        "  if y == 1:\n",
        "    return -np.log(yhat)\n",
        "\n",
        "y = 1 #@param [0, 1] {type:\"raw\"}\n",
        "yhat = 0.47 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "print(f'Loss: {binary_crossentropy(y, yhat)}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.7550225842780328\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8SHAYL25F4M",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "![NN](https://drive.google.com/uc?export=view&id=1EHA2P4kLUQm_FkpYskyJ6QTSskjiaSeo)\n",
        "\n",
        "[2]\n",
        "\n",
        "Example of how a neural network can be visualized!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67hH3A5lxhje",
        "colab_type": "text"
      },
      "source": [
        "# Code\n",
        "\n",
        "The following code is us building a neural network from scratch on the Wisconsin Breast Cancer dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35rh1sKy_kTV",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3zOI-0YYVDb",
        "colab_type": "text"
      },
      "source": [
        "We begin the neural network here by importing some necessary libraries that will allow us to actually create the virtual NN, and also display what goes on internally to maximize the accuracy of the NN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA9v5T8kYxzz",
        "colab_type": "text"
      },
      "source": [
        "- What is the purpose of each library?\n",
        "\n",
        "  - The tensorflow and sklearn libraries are used to properly initialize the neural network and the necessary algorithm needed. \n",
        "  - From the sklearn library, we import the breast cancer dataset. \n",
        "  - We import matplotlib, pandas, and numpy which help us organize and visualize the data and outputs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI5bKzyq15QD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading in the data\n",
        "import sklearn\n",
        "from sklearn.datasets import load_breast_cancer \n",
        "\n",
        "# Visualization\n",
        "import matplotlib as mpl   \n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Building the network \n",
        "import numpy as np\n",
        "\n",
        "# Progress Bar\n",
        "import tqdm as tqdm\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18UQ5KSfAhDn",
        "colab_type": "text"
      },
      "source": [
        "## Loading Dataset, Preprocessing, Visualizing data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1loUILA4N48",
        "colab_type": "text"
      },
      "source": [
        "Adjust the slider to view different portions of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ2LJEnSaYv7",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "outputId": "5438ade2-b50a-41cd-aa18-5004e28af792"
      },
      "source": [
        "data = load_breast_cancer() #read data from sklearn, documentation: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n",
        "full_data = data.data #input features\n",
        "full_target = data.target #labels\n",
        "full_df = pd.DataFrame(full_data, columns=data.feature_names) #convert to panda dataframe\n",
        "full_df['target'] = full_target\n",
        "start_index = 163 #@param {type:\"slider\", min:0, max:564, step:1}\n",
        "full_df[start_index:start_index+5]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>radius error</th>\n",
              "      <th>texture error</th>\n",
              "      <th>perimeter error</th>\n",
              "      <th>area error</th>\n",
              "      <th>smoothness error</th>\n",
              "      <th>compactness error</th>\n",
              "      <th>concavity error</th>\n",
              "      <th>concave points error</th>\n",
              "      <th>symmetry error</th>\n",
              "      <th>fractal dimension error</th>\n",
              "      <th>worst radius</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>12.34</td>\n",
              "      <td>22.22</td>\n",
              "      <td>79.85</td>\n",
              "      <td>464.5</td>\n",
              "      <td>0.10120</td>\n",
              "      <td>0.10150</td>\n",
              "      <td>0.05370</td>\n",
              "      <td>0.02822</td>\n",
              "      <td>0.1551</td>\n",
              "      <td>0.06761</td>\n",
              "      <td>0.2949</td>\n",
              "      <td>1.6560</td>\n",
              "      <td>1.955</td>\n",
              "      <td>21.55</td>\n",
              "      <td>0.011340</td>\n",
              "      <td>0.031750</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.011350</td>\n",
              "      <td>0.01879</td>\n",
              "      <td>0.005348</td>\n",
              "      <td>13.58</td>\n",
              "      <td>28.68</td>\n",
              "      <td>87.36</td>\n",
              "      <td>553.0</td>\n",
              "      <td>0.1452</td>\n",
              "      <td>0.23380</td>\n",
              "      <td>0.1688</td>\n",
              "      <td>0.08194</td>\n",
              "      <td>0.2268</td>\n",
              "      <td>0.09082</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>23.27</td>\n",
              "      <td>22.04</td>\n",
              "      <td>152.10</td>\n",
              "      <td>1686.0</td>\n",
              "      <td>0.08439</td>\n",
              "      <td>0.11450</td>\n",
              "      <td>0.13240</td>\n",
              "      <td>0.09702</td>\n",
              "      <td>0.1801</td>\n",
              "      <td>0.05553</td>\n",
              "      <td>0.6642</td>\n",
              "      <td>0.8561</td>\n",
              "      <td>4.603</td>\n",
              "      <td>97.85</td>\n",
              "      <td>0.004910</td>\n",
              "      <td>0.025440</td>\n",
              "      <td>0.028220</td>\n",
              "      <td>0.016230</td>\n",
              "      <td>0.01956</td>\n",
              "      <td>0.003740</td>\n",
              "      <td>28.01</td>\n",
              "      <td>28.22</td>\n",
              "      <td>184.20</td>\n",
              "      <td>2403.0</td>\n",
              "      <td>0.1228</td>\n",
              "      <td>0.35830</td>\n",
              "      <td>0.3948</td>\n",
              "      <td>0.23460</td>\n",
              "      <td>0.3589</td>\n",
              "      <td>0.09187</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>14.97</td>\n",
              "      <td>19.76</td>\n",
              "      <td>95.50</td>\n",
              "      <td>690.2</td>\n",
              "      <td>0.08421</td>\n",
              "      <td>0.05352</td>\n",
              "      <td>0.01947</td>\n",
              "      <td>0.01939</td>\n",
              "      <td>0.1515</td>\n",
              "      <td>0.05266</td>\n",
              "      <td>0.1840</td>\n",
              "      <td>1.0650</td>\n",
              "      <td>1.286</td>\n",
              "      <td>16.64</td>\n",
              "      <td>0.003634</td>\n",
              "      <td>0.007983</td>\n",
              "      <td>0.008268</td>\n",
              "      <td>0.006432</td>\n",
              "      <td>0.01924</td>\n",
              "      <td>0.001520</td>\n",
              "      <td>15.98</td>\n",
              "      <td>25.82</td>\n",
              "      <td>102.30</td>\n",
              "      <td>782.1</td>\n",
              "      <td>0.1045</td>\n",
              "      <td>0.09995</td>\n",
              "      <td>0.0775</td>\n",
              "      <td>0.05754</td>\n",
              "      <td>0.2646</td>\n",
              "      <td>0.06085</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>10.80</td>\n",
              "      <td>9.71</td>\n",
              "      <td>68.77</td>\n",
              "      <td>357.6</td>\n",
              "      <td>0.09594</td>\n",
              "      <td>0.05736</td>\n",
              "      <td>0.02531</td>\n",
              "      <td>0.01698</td>\n",
              "      <td>0.1381</td>\n",
              "      <td>0.06400</td>\n",
              "      <td>0.1728</td>\n",
              "      <td>0.4064</td>\n",
              "      <td>1.126</td>\n",
              "      <td>11.48</td>\n",
              "      <td>0.007809</td>\n",
              "      <td>0.009816</td>\n",
              "      <td>0.010990</td>\n",
              "      <td>0.005344</td>\n",
              "      <td>0.01254</td>\n",
              "      <td>0.002120</td>\n",
              "      <td>11.60</td>\n",
              "      <td>12.02</td>\n",
              "      <td>73.66</td>\n",
              "      <td>414.0</td>\n",
              "      <td>0.1436</td>\n",
              "      <td>0.12570</td>\n",
              "      <td>0.1047</td>\n",
              "      <td>0.04603</td>\n",
              "      <td>0.2090</td>\n",
              "      <td>0.07699</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>16.78</td>\n",
              "      <td>18.80</td>\n",
              "      <td>109.30</td>\n",
              "      <td>886.3</td>\n",
              "      <td>0.08865</td>\n",
              "      <td>0.09182</td>\n",
              "      <td>0.08422</td>\n",
              "      <td>0.06576</td>\n",
              "      <td>0.1893</td>\n",
              "      <td>0.05534</td>\n",
              "      <td>0.5990</td>\n",
              "      <td>1.3910</td>\n",
              "      <td>4.129</td>\n",
              "      <td>67.34</td>\n",
              "      <td>0.006123</td>\n",
              "      <td>0.024700</td>\n",
              "      <td>0.026260</td>\n",
              "      <td>0.016040</td>\n",
              "      <td>0.02091</td>\n",
              "      <td>0.003493</td>\n",
              "      <td>20.05</td>\n",
              "      <td>26.30</td>\n",
              "      <td>130.70</td>\n",
              "      <td>1260.0</td>\n",
              "      <td>0.1168</td>\n",
              "      <td>0.21190</td>\n",
              "      <td>0.2318</td>\n",
              "      <td>0.14740</td>\n",
              "      <td>0.2810</td>\n",
              "      <td>0.07228</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     mean radius  mean texture  ...  worst fractal dimension  target\n",
              "163        12.34         22.22  ...                  0.09082       1\n",
              "164        23.27         22.04  ...                  0.09187       0\n",
              "165        14.97         19.76  ...                  0.06085       1\n",
              "166        10.80          9.71  ...                  0.07699       1\n",
              "167        16.78         18.80  ...                  0.07228       0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CxJ61I_GMl2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using the ReLu activation function for the hidden layers\n",
        "def relu(x):\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "# Derivative of ReLu for Forward Propagation\n",
        "def relu_derivative(x):\n",
        "  if x > 0:\n",
        "    return 1\n",
        "  elif x <= 0:\n",
        "    return 0\n",
        "\n",
        "# Binary CrossEntropy for the output layer\n",
        "def binary_crossentropy(y, yhat):\n",
        "  if y == 0:\n",
        "    return -np.log(1.0-yhat)\n",
        "\n",
        "  if y == 1:\n",
        "    return -np.log(yhat)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c-vlYiyOO5s",
        "colab_type": "text"
      },
      "source": [
        "## Forward/Back Prop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMwa0nESOXTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_prop(x):\n",
        "  return 0.01\n",
        "\n",
        "def back_prop(x):\n",
        "  return 0.99"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9CrehG0AmoT",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing the Train/Test loss and Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csOS6bJixisJ",
        "colab_type": "text"
      },
      "source": [
        "# Applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo7Zxg7R154s",
        "colab_type": "text"
      },
      "source": [
        "In this section, we will cover some applications of neural networks. These will be \n",
        "\n",
        "\n",
        "*   Medicine\n",
        "*   Robotics\n",
        "*   Finance\n",
        "*   Understanding Natural Language\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNgXJzBjxhqc",
        "colab_type": "text"
      },
      "source": [
        "# References\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbLzTbkIATBy",
        "colab_type": "text"
      },
      "source": [
        "[1] \n",
        "\n",
        "[2]\n",
        "\n",
        "[3]\n",
        "\n",
        "[4]\n",
        "\n",
        "[5]"
      ]
    }
  ]
}