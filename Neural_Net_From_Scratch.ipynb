{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Net_From_Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SD0313/StartOnAI/blob/master/Neural_Net_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbqPIhRQNWWF",
        "colab_type": "text"
      },
      "source": [
        "#Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srB7hSW32oJu",
        "colab_type": "text"
      },
      "source": [
        "We will cover the following topics in this notebook.\n",
        "\n",
        "\n",
        "*   Theory of how Neural Networks work/learn\n",
        "*   Coded Walkthrough of a Neural Network\n",
        "*   Various Applications of Neural Networks\n",
        "\n",
        "So stay tuned!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOr3gaeovftN",
        "colab_type": "text"
      },
      "source": [
        "# What Are Neural Networks?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESAIluzExhen",
        "colab_type": "text"
      },
      "source": [
        "In simple terms, neural networks are representative of the human brain, and they are specificially made to recognize patterns. They interpret data through various models. The patterns that these models detect are all numerical specifically in the form of vectors. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SamhlETV1ZTd",
        "colab_type": "text"
      },
      "source": [
        "Neural networks are extremely helpful for performing tasks involving clustering and classification. Because of the networks similarity to the human brain, it is able to recognize patterns in unlabeled data.\n",
        "\n",
        "We will start off by investigating the most basic Nueral Network: **The Perceptron**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3dDubF6LUsh",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://tinyurl.com/ybcfd78e\" alt=\"perceptron\" width=\"400\"/>\n",
        "\n",
        "[1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFvT_Nv-AXCN",
        "colab_type": "text"
      },
      "source": [
        "The Perceptron consists of two main components\n",
        "1.   Nuerons ($x_i$)\n",
        "2.   Weights ($w_i$)\n",
        "\n",
        "Perceptrons represent the most basic form of a Nueral Network with only two layers, the input and output layer.  As shown in the diagram above, both layers are joined by weights represented by the arrows. Each individual neuron represents a number. For example, if there are three inputs, the input layer will consist of 3 nuerons plus an additional bias neuron. The importance of the bias ($b$) will become clear later in this tutorial. The output layer simply consists of one neuron in this scenario which represents the number we are attempting to predict. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyhmPbDzUspC",
        "colab_type": "text"
      },
      "source": [
        "**Forward Propogation**\n",
        "\n",
        "The process of going from the input layer to the output is known as Forward Propogation. To simplify the computations, we will use vector notation to represent the input features and the weights.\n",
        "\n",
        "  $\\vec{x}=\\begin{bmatrix}  x_1 & x_2 & ... & x_n\\end{bmatrix}$\n",
        "\n",
        "\n",
        "  $\\vec{w}=\\begin{bmatrix}  w_1 & w_2 & ... & w_n \\end{bmatrix}$\n",
        "\n",
        "  Finally, to get the value of the output neuron, we simply take the dot product of these two vectors and add the bias. \n",
        "\n",
        "  $z=\\vec{x}\\cdot\\vec{w}+b=x_1\\times w_1+x_2\\times w_2+...+x_n\\times w_n+b$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibfm7azdc5-2",
        "colab_type": "text"
      },
      "source": [
        "**The Bias Term**\n",
        "\n",
        "To get a better understanding of this output, lets analyze it with just one input neuron. In other words, our output neuron will store the following.\n",
        "\n",
        "$z=x_1\\times w_1+b$\n",
        "\n",
        "If we visualize this in two dimensional space, we know that this will represent a line with slope $w_1$ and intercept $b$. We can now easily see the role of the bias. Without it, our model would always go through the origin. Now, we can shift our model along the axes giving us more flexibility while training. However, we are still only able to represent linear models. To add non-linearities to our model we use an activation function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJdJxEI9hf9T",
        "colab_type": "text"
      },
      "source": [
        "**Activation Functions**\n",
        "\n",
        "Lets imagine that we are solving a binary classification problem. This means the range of our output $\\hat{y}$ (predicted value) must be $(0, 1)$ since we are predicting a probablity that the input belongs to a certain class. However, the range of a linear equation is $(-\\infty, \\infty)$. Therefore, we must apply some other function to satisfy this constraint. In binary classification problems, the most common activation function is called the sigmoid function. \n",
        "\n",
        "$\\sigma(x)=\\frac{1}{1+e^{-x}}$\n",
        "\n",
        "\n",
        "<img src=\"https://tinyurl.com/ycggxehs\" alt=\"sigmoid_graph\" width=\"400\"/>\n",
        "\n",
        "As you can see in this graph, $\\sigma(x)\\in(0, 1)$. This activation function makes it possible to predict a probablity for a binary output. As you go further into machine learning, you will see several other activation functions. The most common ones other than sigmoid are ReLU, tanh, and softmax.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB2F37Lp-jHU",
        "colab_type": "text"
      },
      "source": [
        "**The Output**\n",
        "\n",
        "Now that we know all the parts of the perceptron, lets see how to get the final output. After forward propogation, we saw the output was\n",
        "\n",
        "  $z=\\vec{x}\\cdot\\vec{w}+b=x_1\\times w_1+x_2\\times w_2+...+x_n\\times w_n+b$\n",
        "\n",
        "Finally, we must apply the activation function to get our final output.\n",
        "\n",
        "$\\hat{y}=\\sigma(z)$\n",
        "\n",
        "That is all there is to getting the output from a perceptron! To sum it up in three simple steps:\n",
        "\n",
        "\n",
        "\n",
        "1.   Get the dot product of the weights and the input features $(\\vec{x}\\cdot\\vec{w})$.\n",
        "2.   Add the bias $(\\vec{x}\\cdot\\vec{w}+b)$.\n",
        "3.   Apply the activation function and that is the predicted value $(\\hat{y}=\\sigma(\\vec{x}\\cdot\\vec{w}+b))$!\n",
        "\n",
        "So far we know how to take the input values, and return the corresponding output. However, we must adjust the weights to make the network fit the training data. The process of making these adjustments is known as **back propogation**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g-R2gHbEw5R",
        "colab_type": "text"
      },
      "source": [
        "**Back Propogation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8SHAYL25F4M",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "![NN](https://drive.google.com/uc?export=view&id=1EHA2P4kLUQm_FkpYskyJ6QTSskjiaSeo)\n",
        "\n",
        "[2]\n",
        "\n",
        "Example of how a neural network can be visualized!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_4o-fL5xhhC",
        "colab_type": "text"
      },
      "source": [
        "# Theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aQHI-a00zFk",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67hH3A5lxhje",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI5bKzyq15QD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csOS6bJixisJ",
        "colab_type": "text"
      },
      "source": [
        "# Applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo7Zxg7R154s",
        "colab_type": "text"
      },
      "source": [
        "In this section, we will cover some applications of neural networks. These will be \n",
        "\n",
        "\n",
        "*   Medicine\n",
        "*   Robotics\n",
        "*   Finance\n",
        "*   Understanding Natural Language\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNgXJzBjxhqc",
        "colab_type": "text"
      },
      "source": [
        "# References\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbLzTbkIATBy",
        "colab_type": "text"
      },
      "source": [
        "[1] \n",
        "\n",
        "[2]\n",
        "\n",
        "[3]\n",
        "\n",
        "[4]\n",
        "\n",
        "[5]"
      ]
    }
  ]
}